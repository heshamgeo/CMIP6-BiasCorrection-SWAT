{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Extraction for SWAT Input\n",
    "\n",
    "## Overview\n",
    "This script extracts bias-corrected climate data and converts it into SWAT-compatible text files. It processes different climate parameters (precipitation, temperature, humidity, wind, and radiation) for multiple models and scenarios.\n",
    "\n",
    "## Steps Performed:\n",
    "1. **Define Directories**  \n",
    "   - Paths to bias-corrected precipitation data, standardized other climate data, stations shapefile, and output folder.\n",
    "\n",
    "2. **Parameter Configuration**  \n",
    "   - Defines climate parameters and their corresponding SWAT metadata filenames.\n",
    "\n",
    "3. **Set Scenarios and Time Range**  \n",
    "   - Specifies climate scenarios (`ssp245`, `ssp585`) and forecast years (2025â€“2100).\n",
    "\n",
    "4. **Load Grid Points**  \n",
    "   - Reads centroid locations from the shapefile to extract point-based climate data.\n",
    "\n",
    "5. **Data Extraction Process**\n",
    "   - Loops over models, scenarios, and parameters.\n",
    "   - Extracts data for each grid point in the shapefile.\n",
    "   - Selects nearest latitude/longitude index in the NetCDF files.\n",
    "   - Handles precipitation separately from other parameters.\n",
    "   - Saves extracted data into SWAT input files.\n",
    "\n",
    "6. **Save Metadata for SWAT**  \n",
    "   - Generates SWAT-compatible metadata files such as `pcp.txt`, `tmp.txt`, etc.\n",
    "\n",
    "## Instructions for Use:\n",
    "- Ensure the climate datasets and shapefile are available in the specified directories.\n",
    "- Update paths if necessary.\n",
    "- Run the script to generate SWAT input files.\n",
    "\n",
    "### Output:\n",
    "- SWAT-ready text files containing daily climate data.\n",
    "- Metadata files specifying location details.\n",
    "\n",
    "**Final Message:**\n",
    "_\"SWAT data extraction completed.\"_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Define your directories\n",
    "# -------------------------------------------------------------------\n",
    "bias_corrected_root = r\"D:\\Hesham\\WhiteNile\\CMIP6-BiasCorrection-SWAT\\workingfolder\\bias_corrected\"\n",
    "standardized_root   = r\"D:\\Hesham\\WhiteNile\\CMIP6-BiasCorrection-SWAT\\workingfolder\\converted_data\"\n",
    "grid_shapefile_path = r\"D:\\Hesham\\WhiteNile\\CMIP6-BiasCorrection-SWAT\\workingfolder\\stations_shapefile\\Centroid.shp\"  # Add your stations shapefile\n",
    "output_root_folder  = r\"D:\\Hesham\\WhiteNile\\CMIP6-BiasCorrection-SWAT\\workingfolder\\SWAT_INPUT\"   \n",
    "os.makedirs(output_root_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Define parameters and their SWAT metadata file names\n",
    "# -------------------------------------------------------------------\n",
    "parameters = {\n",
    "    \"tasmax_tasmin\": {\"var_name\": (\"tasmax\", \"tasmin\"), \"metadata_name\": \"tmp.txt\"},\n",
    "    \"hurs\":          {\"var_name\": \"hurs\",   \"metadata_name\": \"rh.txt\"},\n",
    "    \"rsds\":          {\"var_name\": \"rsds\",   \"metadata_name\": \"solar.txt\"},\n",
    "    \"sfcWind\":       {\"var_name\": \"sfcWind\",\"metadata_name\": \"wind.txt\"},\n",
    "    \"pr\":            {\"var_name\": \"pr\",     \"metadata_name\": \"pcp.txt\"},  # Precipitation\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Time range and scenarios\n",
    "# -------------------------------------------------------------------\n",
    "scenarios = [\"ssp245\", \"ssp585\"]\n",
    "years = range(2025, 2101)  # 2025 through 2100\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Load grid points from the shapefile\n",
    "# -------------------------------------------------------------------\n",
    "centroids_gdf = gpd.read_file(grid_shapefile_path)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Main loop over each parameter\n",
    "# -------------------------------------------------------------------\n",
    "for param, param_config in parameters.items():\n",
    "    param_metadata = []  # Will store info like [ID, NAME, LAT, LONG, ELEVATION]\n",
    "\n",
    "    # Either specify your models explicitly or list them from the standardized folder\n",
    "    models = [d for d in os.listdir(standardized_root) if os.path.isdir(os.path.join(standardized_root, d))]\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"Processing model: {model}\")\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            print(f\"  Processing scenario: {scenario}\")\n",
    "\n",
    "            # Prepare scenario-specific output folder\n",
    "            scenario_output_folder = os.path.join(output_root_folder, f\"{model}_{scenario}\")\n",
    "            os.makedirs(scenario_output_folder, exist_ok=True)\n",
    "            param_output_folder = os.path.join(scenario_output_folder, param)\n",
    "            os.makedirs(param_output_folder, exist_ok=True)\n",
    "\n",
    "            # -------------------------------------------------------------------\n",
    "            # Loop over each point in the shapefile\n",
    "            # -------------------------------------------------------------------\n",
    "            for index, centroid in centroids_gdf.iterrows():\n",
    "                point_id = index + 1\n",
    "                lon, lat = centroid.geometry.x, centroid.geometry.y\n",
    "                point_name = f\"{param[:3]}{str(point_id).zfill(3)}\"\n",
    "                elevation = 0  # Placeholder if you don't have real elevation data\n",
    "                txt_file_path = os.path.join(param_output_folder, f\"{point_name}.txt\")\n",
    "\n",
    "                # Skip if the file already exists\n",
    "                if os.path.exists(txt_file_path):\n",
    "                    print(f\"File already exists, skipping: {txt_file_path}\")\n",
    "                    continue\n",
    "\n",
    "                # We will accumulate daily values across all years into this list\n",
    "                daily_values = []\n",
    "                start_date = None\n",
    "\n",
    "                # -------------------------------------------------------------------\n",
    "                # Loop over each year from 2025 to 2100\n",
    "                # -------------------------------------------------------------------\n",
    "                for year in years:\n",
    "                    # -------------------------------------------------------------------\n",
    "                    # 5.1 Construct the NetCDF path differently for precipitation vs. others\n",
    "                    # -------------------------------------------------------------------\n",
    "                    if param == \"pr\":\n",
    "                        # Precipitation from bias-corrected directory\n",
    "                        # e.g.: bias_corrected_{model}_{scenario}_{year}.nc\n",
    "                        nc_file_path = os.path.join(\n",
    "                            bias_corrected_root, \n",
    "                            model,\n",
    "                            scenario,\n",
    "                            f\"bias_corrected_{model}_{scenario}_{year}.nc\"\n",
    "                        )\n",
    "                        if not os.path.exists(nc_file_path):\n",
    "                            print(f\"    File not found: {nc_file_path} => skipping precipitation.\")\n",
    "                            continue\n",
    "\n",
    "                        # Just 1 file to open\n",
    "                        ds = xr.open_dataset(nc_file_path)\n",
    "                        data_var = param_config[\"var_name\"]\n",
    "                        \n",
    "                        lon_idx = abs(ds[\"lon\"].values - lon).argmin()\n",
    "                        lat_idx = abs(ds[\"lat\"].values - lat).argmin()\n",
    "                        time_values = ds.indexes[\"time\"]\n",
    "                        data_values = ds[data_var][:, lat_idx, lon_idx].values\n",
    "\n",
    "                    elif param == \"tasmax_tasmin\":\n",
    "                        # We have 2 parameters: tasmax and tasmin\n",
    "                        # We use glob to handle the dynamic portion of the filename\n",
    "                        \n",
    "                        # Example pattern: tasmax_day_{model}_{scenario}_*_{year}_*_clipped.nc\n",
    "                        tasmax_pattern = os.path.join(\n",
    "                            standardized_root, \n",
    "                            model, \n",
    "                            scenario, \n",
    "                            \"tasmax\",\n",
    "                            f\"tasmax_day_{model}_{scenario}_*_{year}_*_clipped.nc\"\n",
    "                        )\n",
    "                        tasmin_pattern = os.path.join(\n",
    "                            standardized_root, \n",
    "                            model, \n",
    "                            scenario, \n",
    "                            \"tasmin\",\n",
    "                            f\"tasmin_day_{model}_{scenario}_*_{year}_*_clipped.nc\"\n",
    "                        )\n",
    "\n",
    "                        tasmax_files = glob.glob(tasmax_pattern)\n",
    "                        tasmin_files = glob.glob(tasmin_pattern)\n",
    "\n",
    "                        if not (tasmax_files and tasmin_files):\n",
    "                            print(f\"    Missing tasmax or tasmin files for year={year}, skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        # Pick the first match if multiple\n",
    "                        tasmax_file = tasmax_files[0]\n",
    "                        tasmin_file = tasmin_files[0]\n",
    "\n",
    "                        tasmax_ds = xr.open_dataset(tasmax_file)\n",
    "                        tasmin_ds = xr.open_dataset(tasmin_file)\n",
    "\n",
    "                        tasmax_var, tasmin_var = param_config[\"var_name\"]\n",
    "\n",
    "                        lon_idx = abs(tasmax_ds[\"lon\"].values - lon).argmin()\n",
    "                        lat_idx = abs(tasmax_ds[\"lat\"].values - lat).argmin()\n",
    "                        time_values = tasmax_ds.indexes[\"time\"]\n",
    "                        tasmax_values = tasmax_ds[tasmax_var][:, lat_idx, lon_idx].values\n",
    "                        tasmin_values = tasmin_ds[tasmin_var][:, lat_idx, lon_idx].values\n",
    "\n",
    "                    else:\n",
    "                        # For the other parameters (hurs, rsds, sfcWind),\n",
    "                        # we use the standardized folder with a wildcard\n",
    "                        # e.g.: hurs_day_{model}_{scenario}_*_{year}_*_clipped.nc\n",
    "                        pattern = os.path.join(\n",
    "                            standardized_root,\n",
    "                            model,\n",
    "                            scenario,\n",
    "                            param,\n",
    "                            f\"{param}_day_{model}_{scenario}_*_{year}_*_clipped.nc\"\n",
    "                        )\n",
    "                        files = glob.glob(pattern)\n",
    "                        if not files:\n",
    "                            print(f\"    No file match for {param}, year={year} => skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        nc_file_path = files[0]  # pick first if multiple\n",
    "                        ds = xr.open_dataset(nc_file_path)\n",
    "\n",
    "                        data_var = param_config[\"var_name\"]\n",
    "                        lon_idx = abs(ds[\"lon\"].values - lon).argmin()\n",
    "                        lat_idx = abs(ds[\"lat\"].values - lat).argmin()\n",
    "                        time_values = ds.indexes[\"time\"]\n",
    "                        data_values = ds[data_var][:, lat_idx, lon_idx].values\n",
    "\n",
    "                    # -------------------------------------------------------------------\n",
    "                    # 5.2 Store daily data\n",
    "                    # -------------------------------------------------------------------\n",
    "                    if start_date is None and len(time_values) > 0:\n",
    "                        # Convert the first time to a string (YYYYMMDD)\n",
    "                        start_date = pd.to_datetime(time_values[0]).strftime(\"%Y%m%d\")\n",
    "\n",
    "                    # If we are in tasmax_tasmin, we have separate arrays\n",
    "                    if param == \"tasmax_tasmin\":\n",
    "                        daily_values.extend([\n",
    "                            f\"{tx:.2f},{tn:.2f}\"\n",
    "                            for tx, tn in zip(tasmax_values, tasmin_values)\n",
    "                        ])\n",
    "                    else:\n",
    "                        # We have a single array (pr, hurs, rsds, sfcWind)\n",
    "                        daily_values.extend([\n",
    "                            f\"{val:.2f}\"\n",
    "                            for val in data_values\n",
    "                        ])\n",
    "\n",
    "                # -------------------------------------------------------------------\n",
    "                # 5.3 Write daily data to the SWAT text file\n",
    "                # -------------------------------------------------------------------\n",
    "                if len(daily_values) > 0:\n",
    "                    with open(txt_file_path, \"w\") as txt_file:\n",
    "                        txt_file.write(f\"{start_date}\\n\")\n",
    "                        txt_file.write(\"\\n\".join(daily_values))\n",
    "                    print(f\"      Data written for point {point_name} => {txt_file_path}\")\n",
    "                else:\n",
    "                    print(f\"      No data to write for point {point_name} ({param}).\")\n",
    "\n",
    "                # Add metadata for this point\n",
    "                param_metadata.append([point_id, point_name, lat, lon, elevation])\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 6. Save parameter-level metadata (e.g., tmp.txt, pcp.txt, etc.)\n",
    "    # -------------------------------------------------------------------\n",
    "    if len(param_metadata) > 0:\n",
    "        metadata_df = pd.DataFrame(param_metadata, columns=[\"ID\", \"NAME\", \"LAT\", \"LONG\", \"ELEVATION\"])\n",
    "        metadata_file_path = os.path.join(output_root_folder, parameters[param][\"metadata_name\"])\n",
    "        metadata_df.to_csv(metadata_file_path, index=False, sep=\",\")\n",
    "        print(f\"Metadata saved for {param}: {metadata_file_path}\")\n",
    "\n",
    "print(\"SWAT data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySWATplus_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
