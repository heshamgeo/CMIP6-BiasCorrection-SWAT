{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup and Configuration for CMIP6 Data Download**\n",
    "This cell sets up the required configurations for downloading CMIP6 climate model data from the NASA AWS S3 bucket.  \n",
    "\n",
    "- **Lists available CMIP6 datasets** from the public **nex-gddp-cmip6** S3 bucket.  \n",
    "- **Defines supported climate models** along with their unique realization patterns (e.g., `r1i1p1f1_gr`).  \n",
    "- **Specifies emission scenarios and time periods**, including historical (`1950–2014`) and future projections (`2015–2100`) for SSP pathways (`ssp126`, `ssp245`, etc.).  \n",
    "- **Lists climate variables** to be downloaded, such as precipitation (`pr`), maximum/minimum temperature (`tasmax`, `tasmin`), and radiation variables (`rsds`, `rlds`).  \n",
    "- **Includes multiple data versions** (e.g., `_v1.2`, `_v1.1`) to ensure compatibility with different CMIP6 dataset releases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time  # For adding delays\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "!aws s3 ls --no-sign-request s3://nex-gddp-cmip6/\n",
    "\n",
    "\n",
    "# Models with their unique patterns\n",
    "models = {\n",
    "    \"EC-Earth3\": \"r1i1p1f1_gr\",\n",
    "    \"EC-Earth3-Veg-LR\": \"r1i1p1f1_gr\",\n",
    "    \"CNRM-CM6-1\": \"r1i1p1f2_gr\",\n",
    "    \"CNRM-ESM2-1\": \"r1i1p1f2_gr\",\n",
    "    \"IPSL-CM6A-LR\": \"r1i1p1f1_gr\",\n",
    "    \"KACE-1-0-G\": \"r1i1p1f1_gr\",\n",
    "    \n",
    "    \"ACCESS-CM2\": \"r1i1p1f1_gn\",\n",
    "    \"ACCESS-ESM1-5\": \"r1i1p1f1_gn\",\n",
    "    \"BCC-CSM2-MR\": \"r1i1p1f1_gn\",\n",
    "    \"CanESM5\": \"r1i1p1f1_gn\",\n",
    "    \"CESM2\": \"r4i1p1f1_gn\",\n",
    "    \"CESM2-WACCM\": \"r3i1p1f1_gn\",\n",
    "    \"CMCC-CM2-SR5\": \"r1i1p1f1_gn\",\n",
    "    \"CMCC-ESM2\": \"r1i1p1f1_gn\",\n",
    "    \"MIROC-ES2L\": \"r1i1p1f2_gn\",\n",
    "    \"MIROC6\": \"r1i1p1f1_gn\",\n",
    "    \"MPI-ESM1-2-HR\": \"r1i1p1f1_gn\",\n",
    "    \"MPI-ESM1-2-LR\": \"r1i1p1f1_gn\",\n",
    "    \"MRI-ESM2-0\": \"r1i1p1f1_gn\",\n",
    "    \"NESM3\": \"r1i1p1f1_gn\",\n",
    "    \"NorESM2-LM\": \"r1i1p1f1_gn\",\n",
    "    \"NorESM2-MM\": \"r1i1p1f1_gn\",\n",
    "    \"TaiESM1\": \"r1i1p1f1_gn\",\n",
    "    \"UKESM1-0-LL\": \"r1i1p1f2_gn\",\n",
    "    \"FGOALS-g3\": \"r3i1p1f1_gn\",\n",
    "    \"GISS-E2-1-G\": \"r1i1p1f2_gn\",\n",
    "    \"HadGEM3-GC31-LL\": \"r1i1p1f3_gn\",\n",
    "    \"HadGEM3-GC31-MM\": \"r1i1p1f3_gn\",\n",
    "    \"IITM-ESM\": \"r1i1p1f1_gn\",\n",
    "    \n",
    "    \"GFDL-CM4\": \"r1i1p1f1_gr1\",\n",
    "    \"GFDL-ESM4\": \"r1i1p1f1_gr1\",\n",
    "    \"INM-CM4-8\": \"r1i1p1f1_gr1\",\n",
    "    \"INM-CM5-0\": \"r1i1p1f1_gr1\",\n",
    "    \"KIOST-ESM\": \"r1i1p1f1_gr1\",\n",
    "\n",
    "    \"GFDL-CM4_gr2\": \"r1i1p1f1_gr2\"\n",
    "}\n",
    "\n",
    "# Define scenarios and their year ranges\n",
    "scenarios = {\n",
    "    \"historical\": range(1950, 2015),\n",
    "    \"ssp126\": range(2015, 2101),\n",
    "    \"ssp245\": range(2015, 2101),\n",
    "    \"ssp370\": range(2015, 2101),\n",
    "    \"ssp585\": range(2015, 2101)\n",
    "}\n",
    "\n",
    "# Variables to download and possible suffixes\n",
    "variables = {\n",
    "    \"pr\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"tasmax\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"tasmin\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"hurs\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"sfcWind\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"rsds\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"rlds\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"huss\": [\"_v1.2\", \"_v1.1\", \"\"],\n",
    "    \"tas\": [\"_v1.2\", \"_v1.1\", \"\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parallel Downloading of CMIP6 Data from AWS S3**\n",
    "This section automates the bulk download of **CMIP6 climate model data** from NASA’s **AWS S3 storage** using **multi-threading** for efficiency.\n",
    "\n",
    "- **Defines the base S3 bucket and output directory** to store downloaded NetCDF files.\n",
    "- **Lists available files in the S3 directory** before attempting downloads.\n",
    "- **Implements parallel downloading using Python’s `ThreadPoolExecutor`** to speed up the process.\n",
    "- **Ensures the latest available file versions (`_v1.2`, `_v1.1`, etc.) are prioritized** for each variable.\n",
    "- **Creates and maintains the folder structure** based on **model, scenario, and variable**.\n",
    "- **Skips existing files** to avoid redundant downloads and reduce bandwidth usage.\n",
    "- **Handles errors gracefully**, ensuring the script continues even if individual downloads fail.\n",
    "\n",
    "At the end of execution, all requested **CMIP6 historical and future projection data** is downloaded and stored in the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base AWS S3 bucket\n",
    "s3_bucket = \"nex-gddp-cmip6/NEX-GDDP-CMIP6\"\n",
    "\n",
    "# Base output directory\n",
    "base_output_dir = \"D:\\Hesham\\WhiteNile\\CMIP6-BiasCorrection-SWAT\\workingfolder\\CMIP6_GDDP-NEX\"\n",
    "\n",
    "\n",
    "# Download parallel\n",
    "# Function to list files in the S3 directory\n",
    "def list_s3_files(bucket, prefix):\n",
    "    try:\n",
    "        command = f\"aws s3 ls --no-sign-request s3://{bucket}/{prefix}/\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return [line.split()[-1] for line in result.stdout.splitlines()]\n",
    "        else:\n",
    "            print(f\"Error listing S3 files: {result.stderr}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to download a file from S3\n",
    "def download_s3_file(bucket, s3_path, local_path):\n",
    "    try:\n",
    "        command = f\"aws s3 cp --no-sign-request s3://{bucket}/{s3_path} \\\"{local_path}\\\"\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Successfully downloaded: {local_path}\")\n",
    "        else:\n",
    "            print(f\"Error downloading {s3_path}: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Function to handle downloading files for provided parameters (used by the thread pool)\n",
    "def handle_download(s3_bucket, model, scenario, realization, variable, year, available_files, base_output_dir):\n",
    "    suffixes = sorted([\"_v1.2\", \"_v1.1\", \"\"], reverse=True)  # Latest versions first\n",
    "    file_found = False\n",
    "    for suffix in suffixes:\n",
    "        for grid in ['gr', 'gr1', 'gr2', 'gn']:\n",
    "            file_name = f\"{variable}_day_{model}_{scenario}_{realization}_{grid}_{year}{suffix}.nc\"\n",
    "            if file_name in available_files:\n",
    "                local_dir = os.path.join(base_output_dir, model, scenario, variable)\n",
    "                local_file = os.path.join(local_dir, file_name)\n",
    "                os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "                if not os.path.exists(local_file):\n",
    "                    print(f\"Downloading {file_name} from S3...\")\n",
    "                    download_s3_file(s3_bucket, f\"{model}/{scenario}/{realization}/{variable}/{file_name}\", local_file)\n",
    "                else:\n",
    "                    print(f\"File already exists, skipping: {local_file}\")\n",
    "                \n",
    "                file_found = True\n",
    "                break\n",
    "        if file_found:\n",
    "            break\n",
    "\n",
    "    if not file_found:\n",
    "        print(f\"No available files for {variable}, {model}, {scenario}, {year}. Skipping.\")\n",
    "    return file_found\n",
    "\n",
    "# Configure the number of concurrent threads\n",
    "max_workers = 5\n",
    "\n",
    "# Main script logic\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = []\n",
    "\n",
    "    for model, pattern in models.items():\n",
    "        for scenario, year_range in scenarios.items():\n",
    "            realization = pattern.split(\"_\")[0]\n",
    "            for variable in variables:\n",
    "                s3_prefix = f\"{model}/{scenario}/{realization}/{variable}\"\n",
    "                available_files = list_s3_files(s3_bucket, s3_prefix)\n",
    "\n",
    "                for year in year_range:\n",
    "                    future = executor.submit(handle_download, s3_bucket, model, scenario, realization, variable, year, available_files, base_output_dir)\n",
    "                    futures.append(future)\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Ensure all futures are completed\n",
    "\n",
    "print(\"All downloads completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clipping CMIP6 NetCDF Files to the Nile Basin Shapefile**\n",
    "This section processes **downloaded CMIP6 NetCDF files** by clipping them to the exact boundary of the **Nile Basin shapefile**.\n",
    "\n",
    "- **Defines input and output directories** for raw and clipped NetCDF files.\n",
    "- **Loads the Nile Basin shapefile** to extract the required spatial extent.\n",
    "- **Loops through all downloaded NetCDF files**, ensuring only `.nc` files are processed.\n",
    "- **Clips each NetCDF file** to match the spatial boundaries of the shapefile.\n",
    "- **Maintains the original folder structure** in the output directory.\n",
    "- **Saves the clipped NetCDF files** with `_clipped.nc` appended to the filename.\n",
    "- **Handles errors gracefully**, ensuring failed files do not interrupt the workflow.\n",
    "\n",
    "At the end of execution, all CMIP6 climate data is **spatially cropped** to your region and stored in the specified output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import rioxarray  # Required for spatial operations\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define paths\n",
    "input_base_dir = r\"D:/Hesham/WhiteNile/CMIP6-BiasCorrection-SWAT/workingfolder/CMIP6_GDDP-NEX\"\n",
    "output_base_dir = r\"D:/Hesham/WhiteNile/CMIP6-BiasCorrection-SWAT/workingfolder/clipped_data\"\n",
    "shapefile_path = r\"D:/Hesham/WhiteNile/CMIP6-BiasCorrection-SWAT/workingfolder/NileBasin/NileBasin.shp\"\n",
    "\n",
    "# Load the shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Function to clip NetCDF file\n",
    "def clip_nc_to_shapefile(nc_path, output_path):\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")  # Ensure correct projection\n",
    "        clipped_ds = ds.rio.clip(gdf.geometry.apply(lambda x: x.__geo_interface__), gdf.crs)\n",
    "        clipped_ds.to_netcdf(output_path)\n",
    "        print(f\"Clipped and saved: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_path}: {e}\")\n",
    "\n",
    "# Loop through downloaded files and clip them\n",
    "for root, _, files in os.walk(input_base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".nc\"):  # Ensure we're working with NetCDF files\n",
    "            input_nc_path = os.path.join(root, file)\n",
    "            \n",
    "            # Create a similar folder structure in the output directory\n",
    "            relative_path = os.path.relpath(root, input_base_dir)\n",
    "            output_dir = os.path.join(output_base_dir, relative_path)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            output_nc_path = os.path.join(output_dir, file.replace(\".nc\", \"_clipped.nc\"))\n",
    "            \n",
    "            # Process the file\n",
    "            clip_nc_to_shapefile(input_nc_path, output_nc_path)\n",
    "\n",
    "print(\"All NetCDF files have been clipped and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySWATplus_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
